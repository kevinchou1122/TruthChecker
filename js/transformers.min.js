/**
 * Stub implementation of the Transformers.js library for Chrome extensions
 * This file avoids using ES module syntax (export/import) to be compatible with service workers
 */
(function(global) {
  // Define pipelines
  global.pipeline = async function(task, model) {
    console.log(`Creating pipeline for task: ${task} using model: ${model}`);
    return async function(inputs) {
      console.log(`Running ${task} pipeline with inputs:`, inputs);
      if (task === 'text-classification') {
        return [{ label: 'LABEL_1', score: 0.78 }];
      } else if (task === 'token-classification') {
        return [{ entity: 'PERSON', word: 'John', score: 0.95 }];
      } else if (task === 'question-answering') {
        return { answer: 'This is a stub answer', score: 0.87 };
      } else {
        return { generated_text: 'This is a stub generated text' };
      }
    };
  };

  // Define TokenizerModel class
  class TokenizerModel {
    constructor(vocab, merges) {
      this.vocab = vocab || {};
      this.merges = merges || [];
    }
  }

  // Define PreTrainedTokenizer class
  class PreTrainedTokenizer {
    constructor(model) {
      this.model = model;
    }

    async encode(text, options) {
      console.log(`Encoding text with options:`, options);
      return {
        input_ids: [101, 2054, 2003, 1037, 4937, 102], // Sample tokens
        attention_mask: [1, 1, 1, 1, 1, 1]
      };
    }

    async decode(ids) {
      console.log(`Decoding ids:`, ids);
      return "This is a stub decoded text";
    }

    async tokenize(text) {
      console.log(`Tokenizing text:`, text);
      return ["[CLS]", "this", "is", "a", "test", "[SEP]"];
    }
  }

  // Define AutoTokenizer
  global.AutoTokenizer = {
    from_pretrained: async function(model, options) {
      console.log(`Loading tokenizer for model: ${model}`);
      console.log(`With options:`, options);

      const stubVocab = {
        "[PAD]": 0,
        "[UNK]": 1,
        "[CLS]": 2,
        "[SEP]": 3,
        "[MASK]": 4,
        "the": 5,
        "a": 6,
        "is": 7,
        "this": 8,
        "test": 9
      };

      const tokenizer = new PreTrainedTokenizer(
        new TokenizerModel(stubVocab, [])
      );

      // Create a function that mimics the tokenizer behavior
      const tokenizerFunction = async function(text, options = {}) {
        console.log(`Tokenizing text: ${text}`);
        console.log(`With options:`, options);

        // Generate some fake token IDs
        const tokens = typeof text === 'string' ? 
          [101, 2054, 2003, 1037, 4937, 102] : 
          Array(text.length).fill([101, 2054, 2003, 1037, 4937, 102]);
        
        const tokenIds = typeof text === 'string' ? tokens : tokens;
        
        // Default mask is all 1s
        const attentionMask = Array.from(tokenIds).map(() => 1);
        
        // Handle batch input
        const isBatch = Array.isArray(text);
        const batchSize = isBatch ? text.length : 1;
        
        // Create result in the expected format based on return_tensors option
        if (options.return_tensors === 'np' || options.return_tensors === 'tf') {
          return {
            input_ids: {
              data: new Int32Array(isBatch ? tokenIds.flat() : tokenIds),
              dims: isBatch ? [batchSize, tokenIds[0].length] : [1, tokenIds.length]
            },
            attention_mask: {
              data: new Int32Array(isBatch ? Array(batchSize).fill(attentionMask).flat() : attentionMask),
              dims: isBatch ? [batchSize, attentionMask.length] : [1, attentionMask.length]
            }
          };
        } else if (options.return_tensors === 'pt') {
          return {
            input_ids: {
              data: isBatch ? tokenIds.flat() : tokenIds,
              shape: isBatch ? [batchSize, tokenIds[0].length] : [1, tokenIds.length]
            },
            attention_mask: {
              data: isBatch ? Array(batchSize).fill(attentionMask).flat() : attentionMask,
              shape: isBatch ? [batchSize, attentionMask.length] : [1, attentionMask.length]
            }
          };
        } else {
          // Default format
          return {
            input_ids: isBatch ? tokenIds : tokenIds,
            attention_mask: isBatch ? Array(batchSize).fill(attentionMask) : attentionMask
          };
        }
      };

      // Add method references
      tokenizerFunction.encode = tokenizer.encode.bind(tokenizer);
      tokenizerFunction.decode = tokenizer.decode.bind(tokenizer);
      tokenizerFunction.tokenize = tokenizer.tokenize.bind(tokenizer);
      
      return tokenizerFunction;
    }
  };

  // Define AutoModel
  global.AutoModel = {
    from_pretrained: async function(model, options) {
      console.log(`Loading model: ${model}`);
      console.log(`With options:`, options);
      return {
        async forward(inputs) {
          console.log("Running forward pass with inputs:", inputs);
          return {
            last_hidden_state: {
              data: new Float32Array(768).fill(0.1),
              dims: [1, 6, 768]
            },
            pooler_output: {
              data: new Float32Array(768).fill(0.1),
              dims: [1, 768]
            }
          };
        }
      };
    }
  };

  // Define specialized model classes
  global.AutoModelForSequenceClassification = {
    from_pretrained: async function(model, options) {
      console.log(`Loading sequence classification model: ${model}`);
      const baseModel = await global.AutoModel.from_pretrained(model, options);
      return {
        ...baseModel,
        async forward(inputs) {
          const baseOutput = await baseModel.forward(inputs);
          return {
            ...baseOutput,
            logits: {
              data: new Float32Array([0.2, 0.8]),
              dims: [1, 2]
            }
          };
        }
      };
    }
  };

  global.AutoModelForTokenClassification = {
    from_pretrained: async function(model, options) {
      console.log(`Loading token classification model: ${model}`);
      const baseModel = await global.AutoModel.from_pretrained(model, options);
      return {
        ...baseModel,
        async forward(inputs) {
          const baseOutput = await baseModel.forward(inputs);
          return {
            ...baseOutput,
            logits: {
              data: new Float32Array(12).fill(0.1),
              dims: [1, 6, 2]
            }
          };
        }
      };
    }
  };

  // Common models
  global.BertModel = global.AutoModel;
  global.BertTokenizer = global.AutoTokenizer;
  global.RobertaModel = global.AutoModel;
  global.RobertaTokenizer = global.AutoTokenizer;
  global.DistilBertModel = global.AutoModel;
  global.DistilBertTokenizer = global.AutoTokenizer;

  // Utilities for preprocessing
  global.env = {
    initializeProgressBar: false
  };

  console.log("Stub Transformers.js library initialized");
})(self);